{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"SKIL: Deep learning model lifecycle management for humans Python client for Skymind's intelligence layer (SKIL) SKIL is an end-to-end deep learning platform. Think of it as a unified front-end for your deep learning training and deployment process. SKIL supports many popular deep learning libraries, such as Keras, TensorFlow and Deeplearning4J. SKIL increases time-to-value of your AI applications by closing the common gap between experiments and production - bringing models to production fast and keeping them there. acting as middleware for all your AI applications. SKIL effectively acts as middleware for your AI applications and solves a range of common production problems of, namely: Install and run anywhere: SKIL integrates with your current cloud provider, custom on-premise solutions and hybrid architectures. Easy distributed training on Spark: Bring your Keras or TensorFlow model and train it on Apache Spark without any overhead. We support a wide variety of distributed data sources and other vital parts of your production stack. Seamless deployment process: With SKIL, your company's machine learning product lifecycle can be as quick as your data scientist\u2019s experimentation cycle. If you set up a SKIL experiment, model deployment is already accounted for, and makes product integration of deep learning models into a production-grade model server simple - batteries included. Built-in reproducibility and compliance: What model and data did you use? Which pre-processing steps were done? Who carried out the experiment? What library versions were used? Which hardware was utilized? SKIL keeps track of all this information for you. Model organisation and versioning: SKIL makes it easy to keep your various experiments organised, without interfering with your workflow. Your models are versioned and can be updated at any point. Keep working as you're used to: SKIL does not impose an entirely new workflow on you or force you into a UI, just stay right where you are. Happy with your experiment and want to deploy it? Tell SKIL to deploy a service. Your prototype works and you want to scale out training with Spark? Tell SKIL to run a training job. Installation To install SKIL itself, head over to skymind.ai . Probably the easiest way to get started is by using docker : docker pull skymindops/skil-ce docker run --rm -it -p 9008:9008 skymindops/skil-ce bash /start-skil.sh SKIL's Python client can be installed from PyPI: pip install skil Getting started: Deploying an object detection app with SKIL in 60 seconds We're going to deploy an object detection model pre-trained with Google TensorFlow. The model we use is the second version of the You Only Look Once (YOLOv2) model trained on the COCO dataset. Download this model from here and store it as yolo.pb . If you haven't done already, install and start SKIL as described in the last section. For this quick example you only need three (self-explanatory) concepts from SKIL. You first create a Model from the model file yolo.pb you just downloaded. This Model becomes a SKIL Service by deploying it to a SKIL Deployment . That's all there is to it: import skil model = skil.Model('yolo.pb', model_id='yolo_42', name='yolo') service = model.deploy(skil.Deployment(), input_names=['input'], output_names=['output']) Your YOLO object detection app is now live and you can send images to it using the detect_objects method of your service . We use OpenCV (imported as cv2 into Python) to load, annotate and write images: import cv2 image = cv2.imread(\"say_yolo_again.jpg\") detection = service.detect_objects(image) image = skil.utils.yolo.annotate_image(image, detection) cv2.imwrite('annotated.jpg', image) This completes your very first SKIL example, but there are many more advanced examples to get you started: Running YOLO against a live web cam Deploying a Keras model as prediction service to SKIL Deploying a TensorFlow model as prediction service to SKIL Using SKIL's CLI to quickly configure models and deployments Deploying a Keras model from a jupyter notebook WIP Run a Spark training job from a simple Keras model WIP Deploy preprocessing steps and a model as a Pipeline service","title":"Home"},{"location":"#skil-deep-learning-model-lifecycle-management-for-humans","text":"","title":"SKIL: Deep learning model lifecycle management for humans"},{"location":"#python-client-for-skyminds-intelligence-layer-skil","text":"SKIL is an end-to-end deep learning platform. Think of it as a unified front-end for your deep learning training and deployment process. SKIL supports many popular deep learning libraries, such as Keras, TensorFlow and Deeplearning4J. SKIL increases time-to-value of your AI applications by closing the common gap between experiments and production - bringing models to production fast and keeping them there. acting as middleware for all your AI applications. SKIL effectively acts as middleware for your AI applications and solves a range of common production problems of, namely: Install and run anywhere: SKIL integrates with your current cloud provider, custom on-premise solutions and hybrid architectures. Easy distributed training on Spark: Bring your Keras or TensorFlow model and train it on Apache Spark without any overhead. We support a wide variety of distributed data sources and other vital parts of your production stack. Seamless deployment process: With SKIL, your company's machine learning product lifecycle can be as quick as your data scientist\u2019s experimentation cycle. If you set up a SKIL experiment, model deployment is already accounted for, and makes product integration of deep learning models into a production-grade model server simple - batteries included. Built-in reproducibility and compliance: What model and data did you use? Which pre-processing steps were done? Who carried out the experiment? What library versions were used? Which hardware was utilized? SKIL keeps track of all this information for you. Model organisation and versioning: SKIL makes it easy to keep your various experiments organised, without interfering with your workflow. Your models are versioned and can be updated at any point. Keep working as you're used to: SKIL does not impose an entirely new workflow on you or force you into a UI, just stay right where you are. Happy with your experiment and want to deploy it? Tell SKIL to deploy a service. Your prototype works and you want to scale out training with Spark? Tell SKIL to run a training job.","title":"Python client for Skymind's intelligence layer (SKIL)"},{"location":"#installation","text":"To install SKIL itself, head over to skymind.ai . Probably the easiest way to get started is by using docker : docker pull skymindops/skil-ce docker run --rm -it -p 9008:9008 skymindops/skil-ce bash /start-skil.sh SKIL's Python client can be installed from PyPI: pip install skil","title":"Installation"},{"location":"#getting-started-deploying-an-object-detection-app-with-skil-in-60-seconds","text":"We're going to deploy an object detection model pre-trained with Google TensorFlow. The model we use is the second version of the You Only Look Once (YOLOv2) model trained on the COCO dataset. Download this model from here and store it as yolo.pb . If you haven't done already, install and start SKIL as described in the last section. For this quick example you only need three (self-explanatory) concepts from SKIL. You first create a Model from the model file yolo.pb you just downloaded. This Model becomes a SKIL Service by deploying it to a SKIL Deployment . That's all there is to it: import skil model = skil.Model('yolo.pb', model_id='yolo_42', name='yolo') service = model.deploy(skil.Deployment(), input_names=['input'], output_names=['output']) Your YOLO object detection app is now live and you can send images to it using the detect_objects method of your service . We use OpenCV (imported as cv2 into Python) to load, annotate and write images: import cv2 image = cv2.imread(\"say_yolo_again.jpg\") detection = service.detect_objects(image) image = skil.utils.yolo.annotate_image(image, detection) cv2.imwrite('annotated.jpg', image) This completes your very first SKIL example, but there are many more advanced examples to get you started: Running YOLO against a live web cam Deploying a Keras model as prediction service to SKIL Deploying a TensorFlow model as prediction service to SKIL Using SKIL's CLI to quickly configure models and deployments Deploying a Keras model from a jupyter notebook WIP Run a Spark training job from a simple Keras model WIP Deploy preprocessing steps and a model as a Pipeline service","title":"Getting started: Deploying an object detection app with SKIL in 60 seconds"},{"location":"core_concepts/","text":"Getting started with SKIL in Python","title":"SKIL core concepts"},{"location":"core_concepts/#getting-started-with-skil-in-python","text":"","title":"Getting started with SKIL in Python"},{"location":"deployment/","text":"SKIL Deployments [source] Deployment skil.deployments.Deployment(skil=None, name=None, deployment_id=None) Deployments operate independently of workspaces to ensure that there are no accidental interruptions or mistakes in a production environment. Arguments: skil : Skil server instance. name : string. Name for the deployment. id : Unique id for the deployment. If None , a unique id will be generated.","title":"Deployments"},{"location":"deployment/#skil-deployments","text":"[source]","title":"SKIL Deployments"},{"location":"deployment/#deployment","text":"skil.deployments.Deployment(skil=None, name=None, deployment_id=None) Deployments operate independently of workspaces to ensure that there are no accidental interruptions or mistakes in a production environment. Arguments: skil : Skil server instance. name : string. Name for the deployment. id : Unique id for the deployment. If None , a unique id will be generated.","title":"Deployment"},{"location":"experiment/","text":"SKIL Experiments [source] Experiment skil.experiments.Experiment(work_space=None, experiment_id=None, name='experiment', description='experiment', verbose=False, create=True) Experiments in SKIL are useful for defining different model configurations, encapsulating training of models, and carrying out different data cleaning tasks. Experiments have a one-to-one relationship with Notebooks and have their own storage mechanism for saving different model configurations when seeking a best candidate. Arguments: work_space : WorkSpace instance. If None a workspace will be created. experiment_id : integer. Unique id for workspace. If None , a unique id will be generated. name : string. Name for the experiment. description : string. Description for the experiment. verbose : boolean. If True , api response will be printed. create : boolean. If True a new experiment will be created.","title":"Experiments"},{"location":"experiment/#skil-experiments","text":"[source]","title":"SKIL Experiments"},{"location":"experiment/#experiment","text":"skil.experiments.Experiment(work_space=None, experiment_id=None, name='experiment', description='experiment', verbose=False, create=True) Experiments in SKIL are useful for defining different model configurations, encapsulating training of models, and carrying out different data cleaning tasks. Experiments have a one-to-one relationship with Notebooks and have their own storage mechanism for saving different model configurations when seeking a best candidate. Arguments: work_space : WorkSpace instance. If None a workspace will be created. experiment_id : integer. Unique id for workspace. If None , a unique id will be generated. name : string. Name for the experiment. description : string. Description for the experiment. verbose : boolean. If True , api response will be printed. create : boolean. If True a new experiment will be created.","title":"Experiment"},{"location":"inference/","text":"","title":"SKIL inference"},{"location":"jobs/","text":"SKIL Jobs [source] InferenceJobConfiguration skil.jobs.InferenceJobConfiguration(skil_model, batch_size, compute_resource, storage_resource, output_path, data_set_provider_class, is_multi_data_set=False, verbose=False) InferenceJobConfiguration Configuration for a SKIL inference job. On top of what you need to specify for a base JobConfiguration, you need to set the batch size for the model as well. Arguments: skil_model : a skil.Model instance batch_size : int, data batch size to run inference with on the model. compute_resource : `skil.resources.compute.ComputeResource' instance, created before running a job. storage_resource : skil.resources.storage.StorageResource instance created before runnning a job output_path : string with path to folder in which job output should be stored. data_set_provider_class : name of the class to be used as DataSetProvider in SKIL is_multi_data_set : boolean, whether data set uses MultiDataSet interface. verbose : boolean, log level. Set True for detailed logging. [source] TrainingJobConfiguration skil.jobs.TrainingJobConfiguration(skil_model, num_epochs, eval_type, eval_data_set_provider_class, compute_resource, storage_resource, output_path, data_set_provider_class, is_multi_data_set=False, ui_url=None, verbose=False) TrainingJobConfiguration Configuration for a SKIL training job. On top of what you need to specify for a base JobConfiguration, you need to set the number of epochs to train for, a (distributed) training configuration and provide information about how to evaluate your model. Arguments: skil_model : a skil.Model instance compute_resource : `skil.resources.compute.ComputeResource' instance, created before running a job. storage_resource : skil.resources.storage.StorageResource instance created before runnning a job output_path : string with path to folder in which job output should be stored. data_set_provider_class : name of the class to be used as DataSetProvider in SKIL is_multi_data_set : boolean, whether data set uses MultiDataSet interface. verbose : boolean, log level. Set True for detailed logging. [source] InferenceJob skil.jobs.InferenceJob(skil, inference_config, job_id=None, create=True) InferenceJob Initialize and run a SKIL inference job. Arguments: inference_config : InferenceJobConfiguration instance job_id : None by default, provide this ID for existing jobs. create : boolean, whether to create a new job or retrieve an existing one. [source] TrainingJob skil.jobs.TrainingJob(skil, training_config, distributed_config=None, job_id=None, create=True) TrainingJob Initialize and run a SKIL training job. Arguments: training_config : TrainingJobConfiguration instance distributed_config : DistributedConfiguration instance job_id : None by default, provide this ID for existing jobs. create : boolean, whether to create a new job or retrieve an existing one.","title":"Jobs"},{"location":"jobs/#skil-jobs","text":"[source]","title":"SKIL Jobs"},{"location":"jobs/#inferencejobconfiguration","text":"skil.jobs.InferenceJobConfiguration(skil_model, batch_size, compute_resource, storage_resource, output_path, data_set_provider_class, is_multi_data_set=False, verbose=False) InferenceJobConfiguration Configuration for a SKIL inference job. On top of what you need to specify for a base JobConfiguration, you need to set the batch size for the model as well. Arguments: skil_model : a skil.Model instance batch_size : int, data batch size to run inference with on the model. compute_resource : `skil.resources.compute.ComputeResource' instance, created before running a job. storage_resource : skil.resources.storage.StorageResource instance created before runnning a job output_path : string with path to folder in which job output should be stored. data_set_provider_class : name of the class to be used as DataSetProvider in SKIL is_multi_data_set : boolean, whether data set uses MultiDataSet interface. verbose : boolean, log level. Set True for detailed logging. [source]","title":"InferenceJobConfiguration"},{"location":"jobs/#trainingjobconfiguration","text":"skil.jobs.TrainingJobConfiguration(skil_model, num_epochs, eval_type, eval_data_set_provider_class, compute_resource, storage_resource, output_path, data_set_provider_class, is_multi_data_set=False, ui_url=None, verbose=False) TrainingJobConfiguration Configuration for a SKIL training job. On top of what you need to specify for a base JobConfiguration, you need to set the number of epochs to train for, a (distributed) training configuration and provide information about how to evaluate your model. Arguments: skil_model : a skil.Model instance compute_resource : `skil.resources.compute.ComputeResource' instance, created before running a job. storage_resource : skil.resources.storage.StorageResource instance created before runnning a job output_path : string with path to folder in which job output should be stored. data_set_provider_class : name of the class to be used as DataSetProvider in SKIL is_multi_data_set : boolean, whether data set uses MultiDataSet interface. verbose : boolean, log level. Set True for detailed logging. [source]","title":"TrainingJobConfiguration"},{"location":"jobs/#inferencejob","text":"skil.jobs.InferenceJob(skil, inference_config, job_id=None, create=True) InferenceJob Initialize and run a SKIL inference job. Arguments: inference_config : InferenceJobConfiguration instance job_id : None by default, provide this ID for existing jobs. create : boolean, whether to create a new job or retrieve an existing one. [source]","title":"InferenceJob"},{"location":"jobs/#trainingjob","text":"skil.jobs.TrainingJob(skil, training_config, distributed_config=None, job_id=None, create=True) TrainingJob Initialize and run a SKIL training job. Arguments: training_config : TrainingJobConfiguration instance distributed_config : DistributedConfiguration instance job_id : None by default, provide this ID for existing jobs. create : boolean, whether to create a new job or retrieve an existing one.","title":"TrainingJob"},{"location":"model/","text":"SKIL Models [source] Model skil.models.Model(model=None, model_id=None, name=None, version=None, experiment=None, labels='', verbose=False, create=True) SKIL wrapper for DL4J, Keras, TensorFlow and other models SKIL has a robust model storage, serving, and import system for supporting major deep learning libraries. SKIL can be used for end-to-end training, configuration, and deployment of models or alternatively you can import models into SKIL. Arguments model : Model file path or Keras model instance model_id : integer. Unique id for model. If None , a unique id will be generated. name : string. Name for the model. version : integer. Version of the model. Defaults to 1. experiment : Experiment instance. If None , an Experiment object will be created internally. labels : string. Labels associated with the workspace, useful for searching (comma seperated). verbose : boolean. If True , prints api response. create : boolean. Internal. Do not use. [source] Transform skil.models.Transform(transform=None, transform_type='CSV', transform_id=None, name=None, version=None, experiment=None, labels='', verbose=False, create=True) SKIL wrapper for for preprocessing (transform) steps. Currently only supports TransformProcess instances from pydatavec or their serialized versions (JSON format). Arguments transform : pydatavec.TransformProcess or TransformProcess JSON transform_id : integer. Unique id for the transform. If None , a unique id will be generated. transform_type : Type of the SKIL transform. Choose from \"CSV\", \"image\" or \"array\" name : string. Name for the transform. version : integer. Version of the transform. Defaults to 1. experiment : Experiment instance. If None , an Experiment object will be created internally. labels : string. Labels associated with the workspace, useful for searching (comma seperated). verbose : boolean. If True , prints api response. create : boolean. Internal. Do not use.","title":"Models"},{"location":"model/#skil-models","text":"[source]","title":"SKIL Models"},{"location":"model/#model","text":"skil.models.Model(model=None, model_id=None, name=None, version=None, experiment=None, labels='', verbose=False, create=True) SKIL wrapper for DL4J, Keras, TensorFlow and other models SKIL has a robust model storage, serving, and import system for supporting major deep learning libraries. SKIL can be used for end-to-end training, configuration, and deployment of models or alternatively you can import models into SKIL. Arguments model : Model file path or Keras model instance model_id : integer. Unique id for model. If None , a unique id will be generated. name : string. Name for the model. version : integer. Version of the model. Defaults to 1. experiment : Experiment instance. If None , an Experiment object will be created internally. labels : string. Labels associated with the workspace, useful for searching (comma seperated). verbose : boolean. If True , prints api response. create : boolean. Internal. Do not use. [source]","title":"Model"},{"location":"model/#transform","text":"skil.models.Transform(transform=None, transform_type='CSV', transform_id=None, name=None, version=None, experiment=None, labels='', verbose=False, create=True) SKIL wrapper for for preprocessing (transform) steps. Currently only supports TransformProcess instances from pydatavec or their serialized versions (JSON format). Arguments transform : pydatavec.TransformProcess or TransformProcess JSON transform_id : integer. Unique id for the transform. If None , a unique id will be generated. transform_type : Type of the SKIL transform. Choose from \"CSV\", \"image\" or \"array\" name : string. Name for the transform. version : integer. Version of the transform. Defaults to 1. experiment : Experiment instance. If None , an Experiment object will be created internally. labels : string. Labels associated with the workspace, useful for searching (comma seperated). verbose : boolean. If True , prints api response. create : boolean. Internal. Do not use.","title":"Transform"},{"location":"resources/","text":"SKIL compute and storage resources [source] DataProc skil.resources.compute.DataProc(skil, name, project_id, region, spark_cluster_name, credential_uri, resource_id=None, create=True) DataProc Google cloud engine DataProc compute resource Arguments: skil : Skil server instance name : Resource name project_id : GCE project ID region : GCE region cluster_name : DataProc cluster name resource_id : optional resource ID to retrieve an existing resource create : boolean, for internal use only. whether to create a new resource or not [source] EMR skil.resources.compute.EMR(skil, name, region, credential_uri, cluster_id=None, resource_id=None, create=True) EMR AWS Elastic Map Reduce compute resource Arguments: skil : Skil server instance name : Name of the resource region : AWS region of the EMR cluster credential_uri : path to credential file cluster_id : ID of the EMR cluster resource_id : optional resource ID to retrieve an existing resource create : boolean, for internal use only. whether to create a new resource or not [source] HDInsight skil.resources.compute.HDInsight(skil, name, subscription_id, resource_group_name, cluster_name, credential_uri, resource_id=None, create=True) HDInsight Azure HDInsight compute resource. Arguments: skil : Skil server instance name : Resource name subscription_id : Azure subscription ID resource_group_name : Resource group name # TODO: is this SKIL or Azure? cluster_name : HDInsight cluster name resource_id : optional resource ID to retrieve an existing resource create : boolean, for internal use only. whether to create a new resource or not [source] YARN skil.resources.compute.YARN(skil, name, local_spark_home, credential_uri, resource_id=None, create=True) YARN YARN compute resource for local Spark computation on YARN. Arguments: skil : Skil server instance name : Resource name local_spark_home : full path to local Spark binary resource_id : optional resource ID to retrieve an existing resource create : boolean, for internal use only. whether to create a new resource or not [source] AzureStorage skil.resources.storage.AzureStorage(skil, name, container_name, credential_uri, resource_id=None, create=True) AzureStorage SKIL Azure storage resource. Arguments: skil : Skil server instance name : Resource name container_name : Azure storage container name resource_id : optional resource ID to retrieve an existing resource create : boolean, for internal use only. whether to create a new resource or not [source] GoogleStorage skil.resources.storage.GoogleStorage(skil, name, project_id, bucket_name, credential_uri, resource_id=None, create=True) GoogleStorage SKIL Google storage resource. Arguments: skil : Skil server instance name : Resource name project_id : Google project ID bucket_name : bucket name resource_id : optional resource ID to retrieve an existing resource create : boolean, for internal use only. whether to create a new resource or not [source] HDFS skil.resources.storage.HDFS(skil, name, name_node_host, name_node_port, credential_uri, resource_id=None, create=True) HDFS SKIL HDFS resource. Arguments: skil : Skil server instance name : Resource name name_node_host : host of the name node name_node_port : port of the name node resource_id : optional resource ID to retrieve an existing resource create : boolean, for internal use only. whether to create a new resource or not [source] S3 skil.resources.storage.S3(skil, name, bucket, region, credential_uri, resource_id=None, create=True) S3 SKIL S3 resource. Arguments: skil : Skil server instance name : Resource name bucket : S3 bucket name region : AWS region resource_id : optional resource ID to retrieve an existing resource create : boolean, for internal use only. whether to create a new resource or not","title":"Resources"},{"location":"resources/#skil-compute-and-storage-resources","text":"[source]","title":"SKIL compute and storage resources"},{"location":"resources/#dataproc","text":"skil.resources.compute.DataProc(skil, name, project_id, region, spark_cluster_name, credential_uri, resource_id=None, create=True) DataProc Google cloud engine DataProc compute resource Arguments: skil : Skil server instance name : Resource name project_id : GCE project ID region : GCE region cluster_name : DataProc cluster name resource_id : optional resource ID to retrieve an existing resource create : boolean, for internal use only. whether to create a new resource or not [source]","title":"DataProc"},{"location":"resources/#emr","text":"skil.resources.compute.EMR(skil, name, region, credential_uri, cluster_id=None, resource_id=None, create=True) EMR AWS Elastic Map Reduce compute resource Arguments: skil : Skil server instance name : Name of the resource region : AWS region of the EMR cluster credential_uri : path to credential file cluster_id : ID of the EMR cluster resource_id : optional resource ID to retrieve an existing resource create : boolean, for internal use only. whether to create a new resource or not [source]","title":"EMR"},{"location":"resources/#hdinsight","text":"skil.resources.compute.HDInsight(skil, name, subscription_id, resource_group_name, cluster_name, credential_uri, resource_id=None, create=True) HDInsight Azure HDInsight compute resource. Arguments: skil : Skil server instance name : Resource name subscription_id : Azure subscription ID resource_group_name : Resource group name # TODO: is this SKIL or Azure? cluster_name : HDInsight cluster name resource_id : optional resource ID to retrieve an existing resource create : boolean, for internal use only. whether to create a new resource or not [source]","title":"HDInsight"},{"location":"resources/#yarn","text":"skil.resources.compute.YARN(skil, name, local_spark_home, credential_uri, resource_id=None, create=True) YARN YARN compute resource for local Spark computation on YARN. Arguments: skil : Skil server instance name : Resource name local_spark_home : full path to local Spark binary resource_id : optional resource ID to retrieve an existing resource create : boolean, for internal use only. whether to create a new resource or not [source]","title":"YARN"},{"location":"resources/#azurestorage","text":"skil.resources.storage.AzureStorage(skil, name, container_name, credential_uri, resource_id=None, create=True) AzureStorage SKIL Azure storage resource. Arguments: skil : Skil server instance name : Resource name container_name : Azure storage container name resource_id : optional resource ID to retrieve an existing resource create : boolean, for internal use only. whether to create a new resource or not [source]","title":"AzureStorage"},{"location":"resources/#googlestorage","text":"skil.resources.storage.GoogleStorage(skil, name, project_id, bucket_name, credential_uri, resource_id=None, create=True) GoogleStorage SKIL Google storage resource. Arguments: skil : Skil server instance name : Resource name project_id : Google project ID bucket_name : bucket name resource_id : optional resource ID to retrieve an existing resource create : boolean, for internal use only. whether to create a new resource or not [source]","title":"GoogleStorage"},{"location":"resources/#hdfs","text":"skil.resources.storage.HDFS(skil, name, name_node_host, name_node_port, credential_uri, resource_id=None, create=True) HDFS SKIL HDFS resource. Arguments: skil : Skil server instance name : Resource name name_node_host : host of the name node name_node_port : port of the name node resource_id : optional resource ID to retrieve an existing resource create : boolean, for internal use only. whether to create a new resource or not [source]","title":"HDFS"},{"location":"resources/#s3","text":"skil.resources.storage.S3(skil, name, bucket, region, credential_uri, resource_id=None, create=True) S3 SKIL S3 resource. Arguments: skil : Skil server instance name : Resource name bucket : S3 bucket name region : AWS region resource_id : optional resource ID to retrieve an existing resource create : boolean, for internal use only. whether to create a new resource or not","title":"S3"},{"location":"service/","text":"SKIL Services [source] Service skil.services.Service(skil, model, deployment, model_deployment) A service is a deployed model. Arguments: skil : Skil server instance model : skil.Model instance deployment : skil.Deployment instance model_deployment : result of deploy_model API call of a model [source] TransformCsvService skil.services.TransformCsvService(skil, model, deployment, model_deployment) TransformCsvService A service for transforming CSV data Arguments: skil : Skil server instance model : skil.Model instance deployment : skil.Deployment instance model_deployment : result of deploy_model API call of a model [source] TransformArrayService skil.services.TransformArrayService(skil, model, deployment, model_deployment) A service for transforming array data Arguments: skil : Skil server instance model : skil.Model instance deployment : skil.Deployment instance model_deployment : result of deploy_model API call of a model [source] TransformArrayService skil.services.TransformArrayService(skil, model, deployment, model_deployment) A service for transforming array data Arguments: skil : Skil server instance model : skil.Model instance deployment : skil.Deployment instance model_deployment : result of deploy_model API call of a model","title":"Services"},{"location":"service/#skil-services","text":"[source]","title":"SKIL Services"},{"location":"service/#service","text":"skil.services.Service(skil, model, deployment, model_deployment) A service is a deployed model. Arguments: skil : Skil server instance model : skil.Model instance deployment : skil.Deployment instance model_deployment : result of deploy_model API call of a model [source]","title":"Service"},{"location":"service/#transformcsvservice","text":"skil.services.TransformCsvService(skil, model, deployment, model_deployment) TransformCsvService A service for transforming CSV data Arguments: skil : Skil server instance model : skil.Model instance deployment : skil.Deployment instance model_deployment : result of deploy_model API call of a model [source]","title":"TransformCsvService"},{"location":"service/#transformarrayservice","text":"skil.services.TransformArrayService(skil, model, deployment, model_deployment) A service for transforming array data Arguments: skil : Skil server instance model : skil.Model instance deployment : skil.Deployment instance model_deployment : result of deploy_model API call of a model [source]","title":"TransformArrayService"},{"location":"service/#transformarrayservice_1","text":"skil.services.TransformArrayService(skil, model, deployment, model_deployment) A service for transforming array data Arguments: skil : Skil server instance model : skil.Model instance deployment : skil.Deployment instance model_deployment : result of deploy_model API call of a model","title":"TransformArrayService"},{"location":"skil/","text":"[source] Skil skil.base.Skil(workspace_server_id=None, host='localhost', port=9008, debug=False, user_id='admin', password='admin') Central class for managing connections with the SKIL server. Arguments workspace_server_id : None by default, only specify if you want to connect to a non-default SKIL workspace server. host : string, Host on which the SKIL server runs. port : integer, Port on which the SKIL host runs. debug : boolean, set to false for more verbose logging. user_id : user name for your SKIL server connection. password : password of the provided SKIL user.","title":"SKIL servers"},{"location":"skil/#skil","text":"skil.base.Skil(workspace_server_id=None, host='localhost', port=9008, debug=False, user_id='admin', password='admin') Central class for managing connections with the SKIL server. Arguments workspace_server_id : None by default, only specify if you want to connect to a non-default SKIL workspace server. host : string, Host on which the SKIL server runs. port : integer, Port on which the SKIL host runs. debug : boolean, set to false for more verbose logging. user_id : user name for your SKIL server connection. password : password of the provided SKIL user.","title":"Skil"},{"location":"spark/","text":"[source] ParameterAveraging skil.spark.ParameterAveraging(num_workers, batch_size, averaging_frequency=5, num_batches_prefetch=0, collect_stats=False) Parameter averaging configuration for distributed training. Arguments: num_workers : number of Spark workers/executors. batch_size : batch size used for model training averaging_frequency : int, after how many batches of training averaging takes place num_batches_prefetch : int, how many batches to pre-fetch, deactivated if 0. collect_stats : boolean, if statistics get collected during training [source] ParameterSharing skil.spark.ParameterSharing(num_workers, batch_size, shake_frequency=0, min_threshold=1e-05, update_threshold=0.001, workers_per_node=-1, num_batches_prefetch=0, step_delay=50, step_trigger=0.05, threshold_step=1e-05, collect_stats=False) Parameter sharing configuration for distributed training. Arguments: num_workers : number of Spark workers/executors. batch_size : batch size used for model training shake_frequency : shake frequency min_threshold : minimum threshold update_threshold : update threshold workers_per_node : workers per node num_batches_prefetch : number of batches to prefetch step_delay : step delay step_trigger : step trigger threshold_step : threshold step collect_stats : boolean, if statistics get collected during training","title":"Distributed training"},{"location":"spark/#parameteraveraging","text":"skil.spark.ParameterAveraging(num_workers, batch_size, averaging_frequency=5, num_batches_prefetch=0, collect_stats=False) Parameter averaging configuration for distributed training. Arguments: num_workers : number of Spark workers/executors. batch_size : batch size used for model training averaging_frequency : int, after how many batches of training averaging takes place num_batches_prefetch : int, how many batches to pre-fetch, deactivated if 0. collect_stats : boolean, if statistics get collected during training [source]","title":"ParameterAveraging"},{"location":"spark/#parametersharing","text":"skil.spark.ParameterSharing(num_workers, batch_size, shake_frequency=0, min_threshold=1e-05, update_threshold=0.001, workers_per_node=-1, num_batches_prefetch=0, step_delay=50, step_trigger=0.05, threshold_step=1e-05, collect_stats=False) Parameter sharing configuration for distributed training. Arguments: num_workers : number of Spark workers/executors. batch_size : batch size used for model training shake_frequency : shake frequency min_threshold : minimum threshold update_threshold : update threshold workers_per_node : workers per node num_batches_prefetch : number of batches to prefetch step_delay : step delay step_trigger : step trigger threshold_step : threshold step collect_stats : boolean, if statistics get collected during training","title":"ParameterSharing"},{"location":"training/","text":"","title":"SKIL training"},{"location":"work_space/","text":"SKIL Workspaces [source] WorkSpace skil.workspaces.WorkSpace(skil=None, name=None, labels=None, verbose=False, create=True) WorkSpace Workspaces are a collection of features that enable different tasks such as conducting experiments, training models, and test different dataset transforms. Workspaces are distinct from Deployments by operating as a space for non-production work. Arguments skil : Skil server instance name : string. Name for the workspace. labels : string. Labels associated with the workspace, useful for searching (comma seperated). verbose : boolean. If True, api response will be printed. create : boolean. Internal, do not use.","title":"Workspaces"},{"location":"work_space/#skil-workspaces","text":"[source]","title":"SKIL Workspaces"},{"location":"work_space/#workspace","text":"skil.workspaces.WorkSpace(skil=None, name=None, labels=None, verbose=False, create=True) WorkSpace Workspaces are a collection of features that enable different tasks such as conducting experiments, training models, and test different dataset transforms. Workspaces are distinct from Deployments by operating as a space for non-production work. Arguments skil : Skil server instance name : string. Name for the workspace. labels : string. Labels associated with the workspace, useful for searching (comma seperated). verbose : boolean. If True, api response will be printed. create : boolean. Internal, do not use.","title":"WorkSpace"}]}